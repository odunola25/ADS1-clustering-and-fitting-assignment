
"""
A python program that create a poster that showcases interesting clusters of 
data using clustering methods and fits models to datasets using curve fitting
"""

# Import the neccessary libraries
import numpy as np                     #
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.api as sm                     # Statistics
from sklearn.cluster import KMeans               # Clustering
from scipy.optimize import curve_fit             # Fitting

# Tools to support clustering
from cluster_tools import map_corr, scaler, backscale
from errors import error_prop     # error functions from errors.py

# Defining a function that loads, clean and return two dataframes


def read_data(filename):
    """
    This Function reads the csv file, cleans the data, transpose and returns 
    two dataframes as output

    Example:

        df1, df2 = read_data('mydata.csv')

    """
    # Load the data
    wdi_data = pd.read_csv(filename)

    # Clean data by replace non-number with pd.NA
    wdi_data = wdi_data.replace('..', pd.NA)

    # Handle Missing Values
    wdi_data = wdi_data.fillna(method='ffill').fillna(method='bfill')

    # Drop Duplicates
    wdi_data = wdi_data.drop_duplicates()

    # Drop Un-needed Column
    wdi_data.drop(["Country Code", "Series Code"], axis=1, inplace=True)

    # Convert the Year Columns to float data type
    # Columns to convert
    cols_to_convert = ['1980', '1985', '1990', '1995',
                       '2000', '2005', '2010', '2015', '2020']
    wdi_data[cols_to_convert] = wdi_data[cols_to_convert].astype(float)

    # Transpose the Data to have indicators as columns
    # Melt the = dataframe to have 'YEAR' as a column
    wdi_transposed = pd.melt(wdi_data,
                             id_vars=['Series Name', 'Country Name'],
                             var_name='Year',
                             value_name='Value')

    # Pivot the melted dataframe to have individual columns
    # for each country
    wdi_transposed = wdi_transposed.pivot_table(
        index=['Year', 'Country Name'],
        columns='Series Name',
        values='Value',
        aggfunc='first')

    # Reset index for a clean structure
    wdi_transposed.reset_index(inplace=True)

    return wdi_data, wdi_transposed


df1, df2 = read_data('cluster_data.csv')

indicator_list = df1['Series Name'].unique()

# Function to generate visualizations telling a visual story of the data

# Correlation using the map_corr function from cluster_tools
correlation_map = map_corr(df2.select_dtypes(include=np.number))

# Function to Cluster data


def cluster_data(df, n_clusters=4):
    """
    K-means clustering.

    Args:
      df: The dataframe to be clustered.
      n_clusters: The number of clusters to generate.

    Returns:
      A dataframe with the cluster labels.
  """
    # Create a copy of the Dataframe to use
    cluster_df = df.copy()

    # Normalize data using the scaler function from cluster tools
    normalized_data, data_min, data_max = scaler(
        cluster_df.select_dtypes(include=np.number))

    # Create a KMeans model.
    model = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')

    # Fit the model to the data.
    model.fit(normalized_data)

    # Get the cluster labels.
    cluster_labels = model.labels_

    # Backscale the cluster centers
    cluster_centers_backscaled = backscale(
        model.cluster_centers_, data_min, data_max)

    # Transform the original data using the fitted model
    cluster_df['Cluster'] = cluster_labels
    cluster_df['Cluster_Center'] = \
        cluster_df['Cluster'].map(lambda x: cluster_centers_backscaled[x])

    return cluster_df, normalized_data


# Filter the data to be parsed into the clustering function
# Select the three least correlated Indicators for clustering,
cluster_df = df2.loc[:, ['Country Name', 'Year', 'CO2 Emissions',
                         'Renewable Energy Consumption %',
                         'Access to electricity %']]

# Select the Year to use
cluster_df = cluster_df[cluster_df['Year'] == '2015']

c_data, n_data = cluster_data(cluster_df)

# Function to visualize the relationship between clusters


def visualize_clusters(clustered_data, cluster_column, indicators):
    """
    Create visualizations for the clusters.

    Args:
        clustered_data: The dataframe with cluster labels.
        cluster_column: The name of the column containing cluster labels.
        indicators: The list of indicators to be explored.

    Returns:
        None
    """
    # Set the style for the plots
    plt.style.use("seaborn-v0_8-whitegrid")

    # Loop through each indicator to create visualizations
    for indicator in indicators:
        plt.figure(figsize=(12, 8))

        num_clusters = 4
        # Create a barplot for each cluster
        for cluster in range(num_clusters):
            cluster_data = clustered_data[clustered_data
                                          [cluster_column] == cluster]
            plt.bar(int(cluster), cluster_data[indicator],
                    label=f"Cluster {cluster}")

        # Set plot labels and title
        plt.xlabel("Cluster", fontsize=16)
        plt.ylabel(f"{indicator}", fontsize=16)
        plt.title(f'Distribution of {indicator} across Clusters',
                  fontsize=16, fontweight='bold')

        # Add legend
        plt.legend()

        # Set x-axis ticks to integers
        plt.xticks(np.arange(num_clusters),
                   [str(i) for i in range(num_clusters)])

        # Show the plot
        plt.show()

    # Create a scatterplot for different combinations of the indicators for
    # Year 2015
    sns.set_style('darkgrid')

    i_list = [0, 1, 2]
    j_list = [2, 0, 1]
    for i in i_list:
        for j in j_list:
            # Use a scatter plot to show clusters and cluster centers
            sns.scatterplot(data=clustered_data, x=indicators[i],
                            y=indicators[j],
                            hue='Cluster', palette='viridis', s=100)

            # Extract cluster centers
            cluster_centers = clustered_data['Cluster_Center'].tolist()

            # Unpack cluster centers into separate lists for each dimension
            center_x, _, center_y = zip(*cluster_centers)

            # Scatter plot of cluster centers using Seaborn
            sns.scatterplot(x=center_x, y=center_y,
                            marker='X', color='red',
                            s=50, edgecolor='black',
                            linewidth=1,
                            label='Central')

            plt.title('Cluster Data with Cluster Centers for 2015')

            # Add custom legend with "Cluster" prefix
            plt.legend(loc='upper left', bbox_to_anchor=(1, 1))

            plt.show()


visualize_clusters(c_data, "Cluster", ['CO2 Emissions',
                                       'Renewable Energy Consumption %',
                                       'Access to electricity %'])

# Compare Indicator Distribution in each cluster
c_data.groupby('Cluster').last().plot(kind='bar', figsize=(12, 8))
plt.title("Indicator Distribution Across CLusters")
plt.ylabel("Value")
plt.show()

# Function to fit model


def fit_model_with_errors(x, y, x_new, func, initial_guess=None, alpha=0.05):
    """
    Fit a model to the dataset using both curve fitting and OLS and include 
    error propagation.

    Args:
        x: Independent variable values.
        y: Dependent variable values.
        func: Function to fit the data using curve fitting.
        initial_guess: Initial guess for the parameters (default is None).
        alpha: Significance level for confidence intervals (default is 0.05).

    Returns:
        popt: Optimal parameters of the fitted curve.
        pcov: Covariance matrix of the parameters.
        predictions_curve_fit: Fitted curve values using curve_fit.
        predictions_ols: Fitted curve values using OLS.
        conf_int_low_curve_fit: Lower bounds of conf int for curve_fit.
        conf_int_high_curve_fit: Upper bounds of conf int for curve_fit.
        conf_int_low_ols: Lower bounds of confidence intervals for OLS.
        conf_int_high_ols: Upper bounds of confidence intervals for OLS.
    """
    # Perform curve fitting
    popt, pcov = curve_fit(func, x, y, p0=initial_guess)

    # Generate predictions using the fitted model (curve_fit)
    predictions_curve_fit = func(x, *popt)

    # Ordinary Least Squares (OLS) model
    X_ols = sm.add_constant(x)  # Add a constant term
    model_ols = sm.OLS(y, X_ols).fit()

    # Generate predictions using OLS model
    predictions_ols = model_ols.predict(X_ols)

    # Predict for future years using curve_fit
    predictions_future_curve_fit = func(x_new, *popt)

    # Predict for future years using OLS model
    X_ols_new = sm.add_constant(x_new)
    predictions_future_ols = model_ols.predict(X_ols_new)

    # Calculate error ranges using error propagation for curve_fit
    sigma_curve_fit = error_prop(x, func, popt, pcov)

    # Calculate confidence intervals for curve_fit
    lowerbound_curve_fit = predictions_curve_fit - sigma_curve_fit
    upperbound_curve_fit = predictions_curve_fit + sigma_curve_fit

    # Calculate confidence intervals for OLS
    conf_int_ols = model_ols.get_prediction(X_ols).conf_int(alpha=alpha)
    lowerbound_ols, upperbound_ols = conf_int_ols[:, 0], conf_int_ols[:, 1]

    return popt, pcov, predictions_curve_fit, predictions_ols, \
        lowerbound_curve_fit, upperbound_curve_fit, \
        lowerbound_ols, upperbound_ols, predictions_future_curve_fit, \
        predictions_future_ols

# Define a linear function for fitting


def linear_function(x, a, b):
    return a * x + b


# Implementation

# Two countries from different Clusters, cluster 0 and cluster 3
country_list = ['Nigeria', 'Hong Kong SAR, China']
fit_indicators = ['CO2 Emissions',
                  'Renewable Energy Consumption %']

for country in country_list:

    fit_data = df2[df2['Country Name'] == country]

    # Loop through each indicator
    for indicator in fit_indicators:
        # Set Independent Variable
        x_data = fit_data[('Year')].astype('int')
        # Set Dependent Variable
        y_data = fit_data[f'{indicator}']

        # Predict for the next 20 Years
        x_new = pd.DataFrame({'Year': np.arange(2020, 2041, 5)})
        x_new = x_new['Year'].astype('int')

        # Fit the model with error propagation
        optimal_params, \
            covariance_matrix, \
            predictions_curve_fit, \
            predictions_ols, \
            conf_int_low_curve_fit, \
            conf_int_high_curve_fit, \
            lower_bound, \
            upper_bound, \
            predictions_future_curve_fit, \
            predictions_future_ols \
            = fit_model_with_errors(x_data, y_data, x_new, linear_function)

        # Plot the Original data and predictions
        sns.lineplot(x=x_data, y=y_data, label='Original Data', color='blue')
        sns.lineplot(x=x_data, y=predictions_curve_fit,
                     label='Curve Fit Predictions', color='red')
        sns.lineplot(x=x_new, y=predictions_future_ols,
                     label='Future Prediction', color='Purple')
        plt.fill_between(x_data, conf_int_low_curve_fit, conf_int_high_curve_fit,
                         color='black', alpha=0.2, label='Confidence Interval')
        plt.xlabel('Year')
        plt.ylabel(f'{indicator}')
        plt.title(f"Past and Future Outlook for {country}")
        plt.legend()
        plt.show()


# Create a DataFrame of the Future Predictions
future_pred = pd.DataFrame({'Year': x_new,
                            'Renewable Energy Consumption':
                            predictions_future_curve_fit})

c_data_g = c_data.groupby('Cluster').last()
